# High-Performance Deep Learning with SLURM

## Overview
This repository provides a guide to deep learning with **PyTorch**, along with best practices for running workloads on an **HPC cluster using SLURM**. It includes:
- **SLURM Job Scheduling**: Guides and scripts for distributed training.
- **Module Management**: Best practices for handling dependencies on HPC clusters.

---

## Repository Structure
```
/01_introduction/
 â”œâ”€â”€ 01_SLURM.md                    # SLURM job scheduling guide
 â”œâ”€â”€ 02_Modules.md                  # Guide on managing modules
 â”œâ”€â”€ 03_slurm_cheatbook.pdf         # SLURM command reference
 â”œâ”€â”€ README.md                      # Project documentation

```

---

## Contents

### ðŸ”¹ SLURM & HPC Topics Covered
  - **Managing Job Queues & Partitions**
  - **Writing & Submitting SLURM Jobs**
  - **Monitoring & Debugging Jobs**
  - **Using SLURM for Distributed Training**
  - **Managing Dependencies with Modules**

### Prerequisites
To effectively use this repository, ensure you have:
- **Python basics**
- **Familiarity with PyTorch**

---

## Additional Resources
ðŸ“š [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)
ðŸ“š [SLURM Official Guide](https://slurm.schedmd.com/documentation.html)
ðŸ“š [Deep Learning Book by Ian Goodfellow](https://www.deeplearningbook.org/)
