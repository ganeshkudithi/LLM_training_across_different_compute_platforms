{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parallelism\n",
    "\n",
    "This notebook introduces data parallelism.\n",
    "\n",
    "## torch.nn.DataParallel\n",
    "\n",
    "`torch.nn.DataParallel` enables training on multiple GPUs within a single machine by splitting input data across devices and synchronizing gradients after each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Forward Pass\n",
    "\n",
    "1. The input mini-batch is **scattered** across the GPUs.\n",
    "2. Model parameters on GPU 0 are **broadcast** to the other GPUs.\n",
    "3. Each device performs the **forward pass** to compute logits.\n",
    "4. The logits from all devices are **gathered** on GPU 0.\n",
    "5. The final **loss** is computed from the gathered logits (with reduction).\n",
    "\n",
    "![](../01_Basics/images/dp_forward.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "The same process can be expressed in code as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\kisho\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.9.1-cp311-cp311-win_amd64.whl.metadata (30 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\kisho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (4.13.2)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Downloading networkx-3.6-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=0.8.5 (from torch)\n",
      "  Downloading fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading markupsafe-3.0.3-cp311-cp311-win_amd64.whl.metadata (2.8 kB)\n",
      "Downloading torch-2.9.1-cp311-cp311-win_amd64.whl (111.0 MB)\n",
      "   ---------------------------------------- 0.0/111.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/111.0 MB 1.4 MB/s eta 0:01:22\n",
      "   ---------------------------------------- 0.1/111.0 MB 1.8 MB/s eta 0:01:03\n",
      "   ---------------------------------------- 0.2/111.0 MB 1.4 MB/s eta 0:01:20\n",
      "   ---------------------------------------- 0.3/111.0 MB 1.7 MB/s eta 0:01:04\n",
      "   ---------------------------------------- 0.4/111.0 MB 1.9 MB/s eta 0:00:58\n",
      "   ---------------------------------------- 0.5/111.0 MB 1.9 MB/s eta 0:00:59\n",
      "   ---------------------------------------- 0.6/111.0 MB 2.0 MB/s eta 0:00:56\n",
      "   ---------------------------------------- 0.7/111.0 MB 2.1 MB/s eta 0:00:53\n",
      "   ---------------------------------------- 0.8/111.0 MB 2.1 MB/s eta 0:00:54\n",
      "   ---------------------------------------- 1.0/111.0 MB 2.2 MB/s eta 0:00:50\n",
      "   ---------------------------------------- 1.1/111.0 MB 2.3 MB/s eta 0:00:49\n",
      "   ---------------------------------------- 1.2/111.0 MB 2.3 MB/s eta 0:00:48\n",
      "    --------------------------------------- 1.4/111.0 MB 2.5 MB/s eta 0:00:45\n",
      "    --------------------------------------- 1.6/111.0 MB 2.6 MB/s eta 0:00:43\n",
      "    --------------------------------------- 1.8/111.0 MB 2.7 MB/s eta 0:00:42\n",
      "    --------------------------------------- 2.0/111.0 MB 2.8 MB/s eta 0:00:40\n",
      "    --------------------------------------- 2.2/111.0 MB 2.9 MB/s eta 0:00:38\n",
      "    --------------------------------------- 2.4/111.0 MB 3.0 MB/s eta 0:00:36\n",
      "    --------------------------------------- 2.6/111.0 MB 3.1 MB/s eta 0:00:35\n",
      "    --------------------------------------- 2.8/111.0 MB 3.1 MB/s eta 0:00:35\n",
      "   - -------------------------------------- 3.0/111.0 MB 3.2 MB/s eta 0:00:34\n",
      "   - -------------------------------------- 3.3/111.0 MB 3.3 MB/s eta 0:00:33\n",
      "   - -------------------------------------- 3.5/111.0 MB 3.4 MB/s eta 0:00:32\n",
      "   - -------------------------------------- 3.7/111.0 MB 3.4 MB/s eta 0:00:32\n",
      "   - -------------------------------------- 3.8/111.0 MB 3.4 MB/s eta 0:00:32\n",
      "   - -------------------------------------- 3.9/111.0 MB 3.4 MB/s eta 0:00:32\n",
      "   - -------------------------------------- 4.2/111.0 MB 3.5 MB/s eta 0:00:31\n",
      "   - -------------------------------------- 4.4/111.0 MB 3.5 MB/s eta 0:00:31\n",
      "   - -------------------------------------- 4.8/111.0 MB 3.6 MB/s eta 0:00:30\n",
      "   - -------------------------------------- 5.1/111.0 MB 3.8 MB/s eta 0:00:28\n",
      "   - -------------------------------------- 5.4/111.0 MB 3.9 MB/s eta 0:00:28\n",
      "   -- ------------------------------------- 5.6/111.0 MB 3.9 MB/s eta 0:00:27\n",
      "   -- ------------------------------------- 6.1/111.0 MB 4.1 MB/s eta 0:00:26\n",
      "   -- ------------------------------------- 6.5/111.0 MB 4.2 MB/s eta 0:00:25\n",
      "   -- ------------------------------------- 6.7/111.0 MB 4.2 MB/s eta 0:00:25\n",
      "   -- ------------------------------------- 7.1/111.0 MB 4.4 MB/s eta 0:00:24\n",
      "   -- ------------------------------------- 7.5/111.0 MB 4.4 MB/s eta 0:00:24\n",
      "   -- ------------------------------------- 7.9/111.0 MB 4.5 MB/s eta 0:00:23\n",
      "   -- ------------------------------------- 8.3/111.0 MB 4.7 MB/s eta 0:00:22\n",
      "   --- ------------------------------------ 8.7/111.0 MB 4.8 MB/s eta 0:00:22\n",
      "   --- ------------------------------------ 9.0/111.0 MB 4.8 MB/s eta 0:00:22\n",
      "   --- ------------------------------------ 9.4/111.0 MB 4.9 MB/s eta 0:00:21\n",
      "   --- ------------------------------------ 9.6/111.0 MB 4.9 MB/s eta 0:00:21\n",
      "   --- ------------------------------------ 10.1/111.0 MB 5.0 MB/s eta 0:00:21\n",
      "   --- ------------------------------------ 10.5/111.0 MB 5.4 MB/s eta 0:00:19\n",
      "   --- ------------------------------------ 11.0/111.0 MB 6.0 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 11.3/111.0 MB 6.2 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 11.8/111.0 MB 6.5 MB/s eta 0:00:16\n",
      "   ---- ----------------------------------- 12.3/111.0 MB 6.9 MB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 12.9/111.0 MB 7.1 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 13.2/111.0 MB 7.5 MB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 13.6/111.0 MB 7.5 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 14.1/111.0 MB 8.0 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 14.6/111.0 MB 8.4 MB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 15.1/111.0 MB 8.7 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 15.7/111.0 MB 8.8 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 16.1/111.0 MB 9.1 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 16.7/111.0 MB 9.2 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 17.2/111.0 MB 9.5 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 17.8/111.0 MB 9.8 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 18.2/111.0 MB 9.8 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 18.7/111.0 MB 9.8 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 19.2/111.0 MB 10.2 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 19.6/111.0 MB 10.2 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 20.2/111.0 MB 10.6 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 20.5/111.0 MB 10.6 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 21.0/111.0 MB 10.7 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 21.6/111.0 MB 10.7 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 22.3/111.0 MB 10.7 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 22.7/111.0 MB 10.7 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 23.2/111.0 MB 10.9 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 23.8/111.0 MB 11.3 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 23.8/111.0 MB 11.3 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 24.2/111.0 MB 10.6 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 24.5/111.0 MB 10.4 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 25.0/111.0 MB 10.2 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 25.4/111.0 MB 10.4 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 25.7/111.0 MB 10.2 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 26.1/111.0 MB 10.2 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 26.6/111.0 MB 10.1 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 27.0/111.0 MB 10.1 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 27.1/111.0 MB 9.4 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 27.8/111.0 MB 9.5 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 28.2/111.0 MB 9.5 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 28.6/111.0 MB 9.6 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 29.1/111.0 MB 9.5 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 29.7/111.0 MB 9.6 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 30.3/111.0 MB 9.5 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 30.7/111.0 MB 9.9 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 31.4/111.0 MB 9.8 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 31.9/111.0 MB 9.8 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 32.4/111.0 MB 9.8 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 32.6/111.0 MB 9.5 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 33.0/111.0 MB 9.5 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 33.5/111.0 MB 9.4 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 34.2/111.0 MB 9.9 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 34.8/111.0 MB 10.1 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 35.3/111.0 MB 10.1 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 35.9/111.0 MB 10.4 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 36.4/111.0 MB 10.4 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 37.0/111.0 MB 10.6 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 37.3/111.0 MB 11.1 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 37.8/111.0 MB 11.1 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 38.2/111.0 MB 10.9 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 38.9/111.0 MB 10.9 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 39.3/111.0 MB 11.1 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 39.5/111.0 MB 10.9 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 40.0/111.0 MB 10.6 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 40.1/111.0 MB 10.4 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 40.1/111.0 MB 10.4 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 40.1/111.0 MB 10.4 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 40.1/111.0 MB 10.4 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 40.1/111.0 MB 10.4 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 40.6/111.0 MB 8.6 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 40.8/111.0 MB 8.4 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 41.5/111.0 MB 8.4 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 42.0/111.0 MB 8.3 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 42.4/111.0 MB 8.4 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 42.9/111.0 MB 8.5 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 43.5/111.0 MB 8.6 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 44.0/111.0 MB 8.6 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 44.4/111.0 MB 8.6 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 45.1/111.0 MB 8.6 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 45.6/111.0 MB 8.6 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 46.1/111.0 MB 8.6 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 46.6/111.0 MB 8.5 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 47.2/111.0 MB 8.6 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 47.9/111.0 MB 8.6 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 48.4/111.0 MB 8.6 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 48.9/111.0 MB 8.7 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 49.6/111.0 MB 8.8 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 50.0/111.0 MB 9.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 50.4/111.0 MB 11.1 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 50.8/111.0 MB 10.9 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 51.2/111.0 MB 11.1 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 51.8/111.0 MB 11.1 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 52.2/111.0 MB 11.1 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 52.8/111.0 MB 11.1 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 53.0/111.0 MB 10.7 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 53.6/111.0 MB 10.9 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 53.9/111.0 MB 10.9 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 54.4/111.0 MB 10.7 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 55.1/111.0 MB 10.7 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 55.6/111.0 MB 10.9 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 56.1/111.0 MB 10.7 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 56.7/111.0 MB 10.9 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 57.4/111.0 MB 10.7 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 57.8/111.0 MB 10.7 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 58.4/111.0 MB 10.7 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 58.9/111.0 MB 10.7 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 59.4/111.0 MB 10.6 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 59.9/111.0 MB 10.7 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 60.5/111.0 MB 11.1 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 61.1/111.0 MB 11.1 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 61.5/111.0 MB 11.1 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 62.0/111.0 MB 11.1 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 62.3/111.0 MB 11.3 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 62.9/111.0 MB 10.9 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 63.4/111.0 MB 11.5 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 63.8/111.0 MB 11.5 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 64.5/111.0 MB 11.7 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 64.9/111.0 MB 11.5 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 65.2/111.0 MB 11.3 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 65.7/111.0 MB 11.1 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 66.2/111.0 MB 11.1 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 66.6/111.0 MB 11.1 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 67.2/111.0 MB 11.1 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 67.9/111.0 MB 11.3 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 68.2/111.0 MB 11.1 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 68.7/111.0 MB 11.1 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 69.3/111.0 MB 11.1 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 69.9/111.0 MB 11.1 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 70.3/111.0 MB 11.1 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 70.9/111.0 MB 11.1 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 71.4/111.0 MB 11.1 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 72.0/111.0 MB 11.3 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 72.6/111.0 MB 11.3 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 73.0/111.0 MB 11.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 73.4/111.0 MB 11.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 73.8/111.0 MB 10.9 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 74.4/111.0 MB 10.9 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 74.8/111.0 MB 10.7 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 75.3/111.0 MB 10.9 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 75.6/111.0 MB 10.6 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 76.4/111.0 MB 10.9 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 76.7/111.0 MB 10.7 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 77.3/111.0 MB 10.9 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 77.7/111.0 MB 10.6 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 78.3/111.0 MB 10.7 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 78.7/111.0 MB 10.6 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 79.3/111.0 MB 10.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 79.6/111.0 MB 10.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 80.0/111.0 MB 10.2 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 80.6/111.0 MB 10.4 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 81.0/111.0 MB 10.2 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 81.5/111.0 MB 10.2 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 81.9/111.0 MB 10.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 82.4/111.0 MB 9.9 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 82.8/111.0 MB 9.9 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 83.4/111.0 MB 10.1 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 83.9/111.0 MB 10.4 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 84.5/111.0 MB 10.4 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 85.1/111.0 MB 10.6 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 85.5/111.0 MB 10.6 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 86.2/111.0 MB 10.9 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 86.6/111.0 MB 10.6 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 87.1/111.0 MB 10.7 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 87.6/111.0 MB 10.7 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 88.2/111.0 MB 11.1 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 88.6/111.0 MB 11.1 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 89.0/111.0 MB 10.9 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 89.4/111.0 MB 10.7 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 90.0/111.0 MB 10.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 90.6/111.0 MB 10.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 91.1/111.0 MB 10.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 91.6/111.0 MB 11.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 92.1/111.0 MB 11.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 92.4/111.0 MB 10.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 92.9/111.0 MB 10.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 93.3/111.0 MB 10.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 93.6/111.0 MB 10.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 94.1/111.0 MB 10.4 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 94.6/111.0 MB 10.4 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 95.2/111.0 MB 10.4 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 95.5/111.0 MB 10.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 96.1/111.0 MB 10.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 96.7/111.0 MB 10.4 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 97.2/111.0 MB 10.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 97.7/111.0 MB 10.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 98.1/111.0 MB 10.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 98.7/111.0 MB 10.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 99.2/111.0 MB 10.4 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 99.7/111.0 MB 10.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 100.3/111.0 MB 10.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 100.6/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 101.0/111.0 MB 10.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 101.5/111.0 MB 10.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 101.9/111.0 MB 10.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 102.3/111.0 MB 10.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 102.6/111.0 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 103.1/111.0 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 103.6/111.0 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 104.2/111.0 MB 10.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 104.7/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 105.3/111.0 MB 10.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 105.4/111.0 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 106.0/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 106.5/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 106.9/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 107.4/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 107.8/111.0 MB 10.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  108.4/111.0 MB 10.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  108.9/111.0 MB 10.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  109.5/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  110.0/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  110.7/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  111.0/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  111.0/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  111.0/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  111.0/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  111.0/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  111.0/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  111.0/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  111.0/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  111.0/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  111.0/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  111.0/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  111.0/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  111.0/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  111.0/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  111.0/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  111.0/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  111.0/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  111.0/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  111.0/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  111.0/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  111.0/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  111.0/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  111.0/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  111.0/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  111.0/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  111.0/111.0 MB 10.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 111.0/111.0 MB 4.8 MB/s eta 0:00:00\n",
      "Downloading fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
      "   ---------------------------------------- 0.0/201.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 201.4/201.4 kB 6.2 MB/s eta 0:00:00\n",
      "Downloading networkx-3.6-py3-none-any.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.4/2.1 MB 13.1 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.9/2.1 MB 11.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.3/2.1 MB 10.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.1/2.1 MB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 8.2 MB/s eta 0:00:00\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.6/6.3 MB 12.9 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.0/6.3 MB 13.0 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.4/6.3 MB 11.0 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.7/6.3 MB 10.9 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.9/6.3 MB 8.8 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.4/6.3 MB 9.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 2.8/6.3 MB 9.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.2/6.3 MB 9.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.2/6.3 MB 9.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.5/6.3 MB 8.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.1/6.3 MB 8.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.4/6.3 MB 8.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.8/6.3 MB 8.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.3/6.3 MB 8.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.8/6.3 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.3/6.3 MB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.3/6.3 MB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.3/6.3 MB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 7.5 MB/s eta 0:00:00\n",
      "Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "   ---------------------------------------- 0.0/134.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 134.9/134.9 kB 4.0 MB/s eta 0:00:00\n",
      "Downloading markupsafe-3.0.3-cp311-cp311-win_amd64.whl (15 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ----------------------- --------------- 327.7/536.2 kB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  532.5/536.2 kB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 536.2/536.2 kB 6.7 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch\n",
      "Successfully installed MarkupSafe-3.0.3 filelock-3.20.0 fsspec-2025.12.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.6 sympy-1.14.0 torch-2.9.1\n"
     ]
    }
   ],
   "source": [
    "! pip install torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def data_parallel(module, inputs, labels, device_ids, output_device):\n",
    "    # Scatter inputs across devices\n",
    "    inputs = nn.parallel.scatter(inputs, device_ids)\n",
    "\n",
    "    # Replicate model on each device\n",
    "    replicas = nn.parallel.replicate(module, device_ids)\n",
    "   \n",
    "    # Run forward pass in parallel\n",
    "    outputs = nn.parallel.parallel_apply(replicas, inputs)\n",
    "\n",
    "    # Gather outputs to a single device\n",
    "    logits = nn.parallel.gather(outputs, output_device)\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Backward Pass\n",
    "\n",
    "1. The computed loss is **scattered** to all devices.\n",
    "2. Each device runs **backward()** to compute gradients.\n",
    "3. All gradients are **reduced** (summed) to GPU 0.\n",
    "4. The model parameters on GPU 0 are updated using the reduced gradients.\n",
    "\n",
    "![](../images/dp_backward.png)\n",
    "\n",
    "\n",
    "#### For clarification\n",
    "- `loss.backward()` computes gradients by backpropagation.\n",
    "- `optimizer.step()` updates parameters using the computed gradients.\n",
    "- Backward computation is more expensive than the update step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. Load dataset\n",
    "datasets = load_dataset(\"multi_nli\").data[\"train\"]\n",
    "datasets = [\n",
    "    {\n",
    "        \"premise\": str(p),\n",
    "        \"hypothesis\": str(h),\n",
    "        \"labels\": l.as_py(),\n",
    "    }\n",
    "    for p, h, l in zip(datasets[2], datasets[5], datasets[9])\n",
    "]\n",
    "\n",
    "# Create DataLoader\n",
    "data_loader = DataLoader(datasets, batch_size=128, num_workers=4)\n",
    "\n",
    "# 2. Load pretrained model and tokenizer\n",
    "model_name = \"bert-base-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model and move to GPU\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3).cuda()\n",
    "\n",
    "# 3. Enable Data Parallelism (Multi-GPU)\n",
    "# device_ids: list of GPU IDs to use\n",
    "# output_device: GPU where outputs are gathered\n",
    "model = nn.DataParallel(model, device_ids=[0, 1, 2, 3], output_device=0)\n",
    "\n",
    "# 4. Optimizer and loss function\n",
    "optimizer = Adam(model.parameters(), lr=3e-5)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "# 5. Training loop\n",
    "for i, data in enumerate(data_loader):\n",
    "\n",
    "    # Clear old gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Tokenize input text\n",
    "    tokens = tokenizer(\n",
    "        data[\"premise\"],\n",
    "        data[\"hypothesis\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Forward pass (automatically spread across GPUs)\n",
    "    logits = model(\n",
    "        input_ids=tokens.input_ids.cuda(),\n",
    "        attention_mask=tokens.attention_mask.cuda(),\n",
    "        return_dict=False\n",
    "    )[0]\n",
    "\n",
    "    # Compute loss\n",
    "    loss = loss_fn(logits, data[\"labels\"].cuda())\n",
    "\n",
    "    # Backward pass (gradients are synchronized automatically)\n",
    "    loss.backward()\n",
    "\n",
    "    # Update model parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print training status\n",
    "    if i % 10 == 0:\n",
    "        print(f\"step:{i}, loss:{loss}\")\n",
    "\n",
    "    # Stop early for demo\n",
    "    if i == 300:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../src/data_parallel.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Training works correctly on multiple GPUs. However, a problem occurs because all **logits** are gathered on GPU 0, which can cause GPU memory imbalance.\n",
    "\n",
    "This can be improved by gathering the **loss** instead of the logits. Since the loss is a scalar, it uses much less memory.  \n",
    "This idea is similar to the `DataParallelCriterion` approach, but it can be implemented more simply by overriding the model‚Äôs `forward()` function.\n",
    "\n",
    "![](../images/dp_forward_2.png)\n",
    "\n",
    "\n",
    "The key idea is to perform **loss computation inside the forward pass**.  \n",
    "Because the forward function runs in parallel across GPUs, computing the loss there ensures that loss calculation and reduction happen in parallel.\n",
    "\n",
    "One side effect is that **loss reduction happens twice**:\n",
    "- First, each GPU reduces its local mini-batch loss.\n",
    "- Then, the reduced losses from all GPUs are combined into a single value.\n",
    "\n",
    "Even with double reduction, this approach is more efficient because it:\n",
    "- Reduces memory usage on GPU 0.\n",
    "- Parallelizes loss computation.\n",
    "- Improves overall balance across devices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "# Standard model that outputs logits\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(768, 3)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        logits = self.linear(inputs)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Parallel model that computes loss inside forward pass\n",
    "class ParallelLossModel(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs, labels):\n",
    "        # Compute logits using base model\n",
    "        logits = super(ParallelLossModel, self).forward(inputs)\n",
    "        \n",
    "        # Compute loss on each GPU independently\n",
    "        loss = nn.CrossEntropyLoss(reduction=\"mean\")(logits, labels)\n",
    "        \n",
    "        # Return loss instead of logits\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, most Hugging Face Transformers models already support computing the loss directly inside the forward pass.\n",
    "\n",
    "By providing the labels to the model‚Äôs `labels` argument, the model returns the loss automatically.  \n",
    "This allows us to use parallel loss computation without writing any custom loss logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/efficient_data_parallel.py\n",
    "\"\"\"\n",
    "\n",
    "# (Steps 1‚Äì4 are omitted for brevity)\n",
    "\n",
    "# 5. Start training loop\n",
    "for i, data in enumerate(data_loader):\n",
    "\n",
    "    # Clear previous gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Tokenize input text\n",
    "    tokens = tokenizer(\n",
    "        data[\"premise\"],\n",
    "        data[\"hypothesis\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Forward pass (model computes loss internally)\n",
    "    loss = model(\n",
    "        input_ids=tokens.input_ids.cuda(),\n",
    "        attention_mask=tokens.attention_mask.cuda(),\n",
    "        labels=data[\"labels\"],\n",
    "    ).loss\n",
    "\n",
    "    # Reduce loss across GPUs (e.g., shape: [num_gpus] ‚Üí [1])\n",
    "    loss = loss.mean()\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print training status\n",
    "    if i % 10 == 0:\n",
    "        print(f\"step:{i}, loss:{loss}\")\n",
    "\n",
    "    # Stop early for demonstration\n",
    "    if i == 300:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../src/efficient_data_parallel.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Limitations of `torch.nn.DataParallel`\n",
    "\n",
    "### 1) Inefficient multi-threading in Python\n",
    "`DataParallel` uses multi-threading, but Python is limited by the **Global Interpreter Lock (GIL)**.  \n",
    "This prevents true parallel execution within a single process.  \n",
    "For better performance, training should use **multi-process execution** instead.\n",
    "\n",
    "### 2) Model replication overhead\n",
    "Gradients are gathered on one GPU and the model is updated there.  \n",
    "After each update, the model must be broadcast back to all GPUs.  \n",
    "This repeated synchronization is expensive and limits scalability.\n",
    "\n",
    "### Solution ‚Üí All-Reduce üëç\n",
    "![](../images/allreduce.png)\n",
    "\n",
    "Instead of collecting gradients on one GPU, **all-reduce** sums gradients across all GPUs and shares the result with every device.  \n",
    "This allows each GPU to update its own copy of the model without rebroadcasting.\n",
    "\n",
    "### However...\n",
    "All-reduce itself is computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. `torch.nn.parallel.DistributedDataParallel` (DDP)\n",
    "\n",
    "### Ring All-Reduce\n",
    "Ring all-reduce is a communication method introduced in 2017 to improve gradient synchronization performance.  \n",
    "It became the foundation of DDP because it is much more efficient than earlier approaches.\n",
    "\n",
    "![](../images/ring_allreduce.gif)\n",
    "\n",
    "\n",
    "### What is DDP?\n",
    "`DistributedDataParallel` (DDP) is a data-parallel training module designed to fix the limitations of `DataParallel`.  \n",
    "It works on both **single-node and multi-node systems** using **multiple processes** instead of threads.\n",
    "\n",
    "By using **all-reduce**, DDP removes the need for a master GPU.  \n",
    "Each process updates its own model, making training faster and more scalable.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/ddp.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. Initialize distributed process group\n",
    "dist.init_process_group(\"nccl\")\n",
    "rank = dist.get_rank()              # Process ID\n",
    "world_size = dist.get_world_size()  # Total number of processes\n",
    "torch.cuda.set_device(rank)         # Assign one GPU per process\n",
    "device = torch.cuda.current_device()\n",
    "\n",
    "# 2. Load dataset\n",
    "datasets = load_dataset(\"multi_nli\").data[\"train\"]\n",
    "datasets = [\n",
    "    {\n",
    "        \"premise\": str(p),\n",
    "        \"hypothesis\": str(h),\n",
    "        \"labels\": l.as_py(),\n",
    "    }\n",
    "    for p, h, l in zip(datasets[2], datasets[5], datasets[9])\n",
    "]\n",
    "\n",
    "# 3. Create DistributedSampler\n",
    "# This splits the dataset across multiple processes\n",
    "sampler = DistributedSampler(\n",
    "    datasets,\n",
    "    num_replicas=world_size,\n",
    "    rank=rank,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    datasets,\n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    "    sampler=sampler,\n",
    "    shuffle=False,     # Must be False when using DistributedSampler\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# 4. Load model and tokenizer\n",
    "model_name = \"bert-base-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3).cuda()\n",
    "\n",
    "# 5. Wrap model with DistributedDataParallel\n",
    "model = DistributedDataParallel(model, device_ids=[device], output_device=device)\n",
    "\n",
    "# 6. Create optimizer\n",
    "optimizer = Adam(model.parameters(), lr=3e-5)\n",
    "\n",
    "# 7. Training loop\n",
    "for i, data in enumerate(data_loader):\n",
    "\n",
    "    # Clear gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Tokenize input text\n",
    "    tokens = tokenizer(\n",
    "        data[\"premise\"],\n",
    "        data[\"hypothesis\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Forward pass (each process runs independently)\n",
    "    loss = model(\n",
    "        input_ids=tokens.input_ids.cuda(),\n",
    "        attention_mask=tokens.attention_mask.cuda(),\n",
    "        labels=data[\"labels\"],\n",
    "    ).loss\n",
    "\n",
    "    # Backpropagation (gradients are synchronized automatically)\n",
    "    loss.backward()\n",
    "\n",
    "    # Update model parameters locally on each GPU\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print only from main process\n",
    "    if i % 10 == 0 and rank == 0:\n",
    "        print(f\"step:{i}, loss:{loss}\")\n",
    "\n",
    "    # Stop early for demonstration\n",
    "    if i == 300:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: `torch.distributed.launch` is deprecated.  \n",
    "Use `torchrun` for newer PyTorch versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m  torch.distributed.launch --nproc_per_node=4 ../src/ddp.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
