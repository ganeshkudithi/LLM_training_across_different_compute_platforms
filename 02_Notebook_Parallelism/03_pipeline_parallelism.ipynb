{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Parallelism\n",
    "\n",
    "In this session, we will learn about **Pipeline Parallelism**.\n",
    "\n",
    "## 1. Inter-layer Model Parallelism\n",
    "\n",
    "Pipeline Parallelism is an improvement over **Inter-layer model parallelism**.\n",
    "\n",
    "In Inter-layer model parallelism, different layers are assigned to different GPUs.  \n",
    "Each group of layers allocated to a GPU is called a **stage**.\n",
    "\n",
    "In the example below:\n",
    "- GPU 1 contains layers (1, 2, 3)\n",
    "- GPU 2 contains layers (4, 5)\n",
    "- The model is divided into **2 stages**\n",
    "\n",
    "<br>\n",
    "\n",
    "![Inter-layer Model Parallelism](../images/inter_layer.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "## Limitation of Inter-layer Model Parallelism\n",
    "\n",
    "Because neural networks depend on the output of previous layers,  \n",
    "the next stage **cannot start until the previous stage finishes**.\n",
    "\n",
    "This results in:\n",
    "\n",
    "- Only one GPU being active at a time  \n",
    "- Other GPUs remaining idle  \n",
    "- Very poor hardware utilization  \n",
    "\n",
    "<br>\n",
    "\n",
    "![Sequential Execution](../images/inter_layer_2.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "![Idle Time Illustration](../images/inter_layer_3.gif)\n",
    "\n",
    "## Key Problem\n",
    "\n",
    "Inter-layer model parallelism behaves like a relay race —  only **one GPU works at any given time**.\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Aspect | Inter-layer Parallelism |\n",
    "|--------|------------------------|\n",
    "| Execution style | Sequential |\n",
    "| GPU utilization | Very low |\n",
    "| Synchronization | Blocking |\n",
    "| Scalability | Poor |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPipe\n",
    "\n",
    "GPipe is a pipeline parallelism technique developed by Google.  \n",
    "It was introduced to reduce GPU idle time in inter-layer model parallelism by further splitting a **mini-batch into micro-batches** and executing them in a pipelined manner.\n",
    "\n",
    "<br>\n",
    "\n",
    "![GPipe Overview](../images/gpipe_1.png)\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### Micro-batch\n",
    "\n",
    "- A **mini-batch** is a subset of the dataset used for one training step.\n",
    "- A **micro-batch** is a further division of a mini-batch into smaller pieces.\n",
    "\n",
    "<br>\n",
    "\n",
    "![Micro-batch Definition](../images/gpipe_2.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### Pipelining\n",
    "\n",
    "GPipe splits mini-batches into micro-batches and schedules them through pipeline stages.\n",
    "\n",
    "The red region in the diagram is called **bubble time**, representing time during which GPUs remain idle.\n",
    "\n",
    "As the micro-batch size increases:\n",
    "- Pipeline utilization improves\n",
    "- Bubble time decreases\n",
    "\n",
    "<br>\n",
    "\n",
    "![Bubble Time Visualization](../images/gpipe_3.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPipe with PyTorch\n",
    "\n",
    "You can easily use GPipe in PyTorch using `torchgpipe`, released by KakaoBrain.\n",
    "\n",
    "However, there are several important limitations:\n",
    "\n",
    "- Only models wrapped with `nn.Sequential` are supported.\n",
    "- Every module’s input and output must be:\n",
    "  - `torch.Tensor` or  \n",
    "  - `Tuple[torch.Tensor]`\n",
    "- Complex model architectures (e.g., branching, multiple inputs) are difficult to implement.\n",
    "\n",
    "As a result, implementing GPipe in practice can be **quite restrictive and challenging**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/gpipe.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchgpipe import GPipe\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Block as GPT2BlockBase\n",
    "\n",
    "\n",
    "class GPT2Preprocessing(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.wte = nn.Embedding(config.vocab_size, self.embed_dim)\n",
    "        self.wpe = nn.Embedding(config.max_position_embeddings, self.embed_dim)\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_shape[-1])\n",
    "        position_ids = torch.arange(\n",
    "            0, input_shape[-1], dtype=torch.long, device=input_ids.device\n",
    "        )\n",
    "        position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])\n",
    "        inputs_embeds = self.wte(input_ids)\n",
    "        position_embeds = self.wpe(position_ids)\n",
    "        hidden_states = inputs_embeds + position_embeds\n",
    "        hidden_states = self.drop(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class GPT2Block(GPT2BlockBase):\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = super(GPT2Block, self).forward(\n",
    "            hidden_states=hidden_states,\n",
    "        )\n",
    "        return hidden_states[0]\n",
    "\n",
    "\n",
    "class GPT2Postprocessing(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_f = nn.LayerNorm(\n",
    "            config.hidden_size,\n",
    "            eps=config.layer_norm_epsilon,\n",
    "        )\n",
    "        self.lm_head = nn.Linear(\n",
    "            config.hidden_size,\n",
    "            config.vocab_size,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.ln_f(hidden_states)\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "        return lm_logits\n",
    "\n",
    "\n",
    "def create_model_from_pretrained(model_name):\n",
    "    pretrained = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    preprocess = GPT2Preprocessing(pretrained.config)\n",
    "    preprocess.wte.weight = pretrained.transformer.wte.weight\n",
    "    preprocess.wpe.weight = pretrained.transformer.wpe.weight\n",
    "\n",
    "    blocks = pretrained.transformer.h\n",
    "    for i, block in enumerate(blocks):\n",
    "        block.__class__ = GPT2Block\n",
    "\n",
    "    postprocess = GPT2Postprocessing(pretrained.config)\n",
    "    postprocess.ln_f.weight = pretrained.transformer.ln_f.weight\n",
    "    postprocess.ln_f.bias = pretrained.transformer.ln_f.bias\n",
    "    postprocess.lm_head.weight.data = pretrained.lm_head.weight.data.clone()\n",
    "\n",
    "    return nn.Sequential(preprocess, *blocks, postprocess)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    world_size = 4\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = create_model_from_pretrained(model_name=\"gpt2\")\n",
    "    model = GPipe(\n",
    "        model,\n",
    "        balance=[4, 3, 3, 4],\n",
    "        devices=[0, 1, 2, 3],\n",
    "        chunks=world_size,\n",
    "    )\n",
    "\n",
    "    datasets = load_dataset(\"squad\").data[\"train\"][\"context\"]\n",
    "    datasets = [str(sample) for sample in datasets]\n",
    "    data_loader = DataLoader(datasets, batch_size=8, num_workers=8)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=3e-5)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for i, data in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        tokens = tokenizer(data, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        input_ids = tokens.input_ids.to(0)\n",
    "        labels = tokens.input_ids.to(world_size - 1)\n",
    "\n",
    "        lm_logits = model(input_ids)\n",
    "        shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        loss = nn.CrossEntropyLoss()(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"step: {i}, loss: {loss}\")\n",
    "        if i == 300:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/home/ubuntu/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 55.94it/s]\n",
      "step: 0, loss: 6.084661483764648\n",
      "step: 10, loss: 3.2574026584625244\n",
      "step: 20, loss: 2.796205759048462\n",
      "step: 30, loss: 2.5538008213043213\n",
      "step: 40, loss: 2.8463237285614014\n",
      "step: 50, loss: 2.3466761112213135\n",
      "step: 60, loss: 2.5407633781433105\n",
      "step: 70, loss: 2.2434418201446533\n",
      "step: 80, loss: 2.4792842864990234\n",
      "step: 90, loss: 2.9400510787963867\n",
      "step: 100, loss: 2.8163280487060547\n",
      "step: 110, loss: 2.4787795543670654\n",
      "step: 120, loss: 2.9588236808776855\n",
      "step: 130, loss: 2.3893203735351562\n",
      "step: 140, loss: 2.9571073055267334\n",
      "step: 150, loss: 3.9219329357147217\n",
      "step: 160, loss: 3.023880958557129\n",
      "step: 170, loss: 3.018484592437744\n",
      "step: 180, loss: 1.6825034618377686\n",
      "step: 190, loss: 3.5461761951446533\n",
      "step: 200, loss: 3.6606838703155518\n",
      "step: 210, loss: 3.527740001678467\n",
      "step: 220, loss: 2.988645315170288\n",
      "step: 230, loss: 3.1758480072021484\n",
      "step: 240, loss: 2.5451812744140625\n",
      "step: 250, loss: 3.1476473808288574\n",
      "step: 260, loss: 3.4633867740631104\n",
      "step: 270, loss: 3.199225902557373\n",
      "step: 280, loss: 2.612720489501953\n",
      "step: 290, loss: 2.139256238937378\n",
      "step: 300, loss: 3.437178373336792\n"
     ]
    }
   ],
   "source": [
    "# !python -m torch.distributed.launch --nproc_per_node=4 ../src/gpipe.py\n",
    "!python ../src/gpipe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 1F1B Pipelining (PipeDream)\n",
    "\n",
    "PipeDream, released by Microsoft, performs pipeline parallelism in a different way from GPipe.  \n",
    "This method is commonly referred to as **1F1B (One Forward One Backward)**.\n",
    "\n",
    "Unlike GPipe, which performs all forward passes first and then starts backward propagation,  \n",
    "PipeDream alternates between forward and backward passes.\n",
    "\n",
    "<img src=\"../images/1f1b.png\" width=600>\n",
    "\n",
    "\n",
    "### Challenges in 1F1B Pipelining\n",
    "\n",
    "1F1B pipelining introduces two major challenges:\n",
    "1. Weight version management  \n",
    "2. Work partitioning  \n",
    "\n",
    "<br>\n",
    "\n",
    "### 1) Weight Version Management\n",
    "\n",
    "GPipe operates using a single version of weights, but periodically performs a **pipeline flush**.\n",
    "\n",
    "Pipeline flush:\n",
    "- Updates parameters using accumulated gradients\n",
    "- During this process, **no forward or backward computation occurs**\n",
    "- Results in reduced GPU utilization\n",
    "\n",
    "<img src=\"../images/pipeline_flush.png\" width=600>\n",
    "\n",
    "PipeDream eliminates pipeline flushes by continuously updating parameters.  \n",
    "As a result, forward and backward passes are always active, significantly improving utilization.\n",
    "\n",
    "However, this introduces another problem:\n",
    "If only the latest weights are stored, then when activations from previous pipeline stages arrive,  \n",
    "downstream stages may already be using **newer weights**, causing inconsistency.\n",
    "\n",
    "\n",
    "To solve this, PipeDream **maintains multiple versions of weights**.  \n",
    "This increases memory usage, leading to a trade-off:\n",
    "\n",
    "- **GPipe**: Memory efficient, Compute inefficient  \n",
    "- **PipeDream**: Compute efficient, Memory inefficient  \n",
    "\n",
    "<br>\n",
    "\n",
    "### 2) Work Partitioning\n",
    "\n",
    "The second challenge is deciding how to partition the neural network.\n",
    "\n",
    "Using the same number of layers per GPU is not always optimal.  \n",
    "The primary goal is to **minimize idle time** across all pipeline stages.\n",
    "\n",
    "To accomplish this:\n",
    "- Each partition should have similar execution time\n",
    "- Parameter size and activation memory must be considered\n",
    "\n",
    "<img src=\"../images/pipe_dream.png\" width=600>\n",
    "\n",
    "PipeDream uses profiling and optimization techniques  \n",
    "to determine an optimal partitioning strategy.\n",
    "\n",
    "<br><br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
