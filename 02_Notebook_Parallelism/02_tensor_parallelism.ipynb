{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Parallelism\n",
    "In this session, we will learn about Tensor Parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Intra-layer Model Parallelism\n",
    "\n",
    "Tensor Parallelism is a form of **intra-layer model parallelism**, where the model is split at the *tensor level within a layer*.  \n",
    "Inter-layer model parallelism is generally intuitive, but intra-layer parallelism can be harder to understand at first.\n",
    "\n",
    "![Intra-layer Parallelism](../images/intra_layer.png)\n",
    "\n",
    "\n",
    "## Why Tensor Parallelism Works\n",
    "\n",
    "Matrix multiplication has a key property:\n",
    "\n",
    "Matrices can be split, computed independently, and then summed or concatenated without changing the final output.\n",
    "\n",
    "This makes it possible to parallelize computation within a single layer.\n",
    "\n",
    "\n",
    "## What is Tensor Parallelism?\n",
    "\n",
    "Tensor Parallelism takes advantage of this property by splitting tensors across GPUs and computing parts of the operation in parallel.\n",
    "\n",
    "\n",
    "## Terminology Clarification\n",
    "\n",
    "The terminology can be confusing:\n",
    "\n",
    "- **Intra-layer Parallelism**  \n",
    "  Any type of parallelism that happens *within a layer*.\n",
    "\n",
    "- **Tensor Parallelism**  \n",
    "  A specific implementation of intra-layer parallelism using tensor slicing and distributed computation.\n",
    "\n",
    "\n",
    "### In Short\n",
    "\n",
    "**Tensor Parallelism = splitting tensors inside a layer to parallelize computation across devices.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of **intra-layer model parallelism** and is one of the most important in large-scale model training today.\n",
    "\n",
    "<img src=\"../images/megatron_lm.jpeg\" width=540>\n",
    "\n",
    "\n",
    "\n",
    "## Column & Row Parallelism\n",
    "\n",
    "Below are illustrations of **Column Parallelism** and **Row Parallelism** used in Megatron-LM.\n",
    "\n",
    "- **Column Parallelism**  \n",
    "  The weight matrix `A` is split **vertically** into submatrices `(A₁, A₂)`.\n",
    "\n",
    "- **Row Parallelism**  \n",
    "  The weight matrix `A` is split **horizontally** into submatrices `(A₁, A₂)`.\n",
    "\n",
    "![Column & Row Parallelism](../images/intra_layer_2.png)\n",
    "\n",
    "\n",
    "## Let’s Implement It with Code\n",
    "\n",
    "Now let’s test the idea in code.\n",
    "\n",
    "First, we compute the matrix multiplication result of tensor `X` and tensor `A`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 74,  98],\n",
      "        [258, 346]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "src/non_parallelism.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "X = torch.tensor(\n",
    "    [\n",
    "        [0, 1, 2, 3],\n",
    "        [4, 5, 6, 7],\n",
    "    ]\n",
    ")\n",
    "\n",
    "A = torch.tensor(\n",
    "    [\n",
    "        [10, 14],\n",
    "        [11, 15],\n",
    "        [12, 16],\n",
    "        [13, 17],        \n",
    "    ]\n",
    ")\n",
    "\n",
    "Y = X @ A\n",
    "\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Parallelism\n",
    "\n",
    "In Column Parallelism, the weight matrix `A` is split **vertically** and the result of each split is **concatenated after computation**.\n",
    "\n",
    "As shown in the figure:\n",
    "- Tensor `X` is **replicated** across devices\n",
    "- Tensor `A` is split vertically into `(A₁, A₂)`\n",
    "- Each device computes:\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 74],\n",
      "        [258]])\n",
      "tensor([[ 98],\n",
      "        [346]])\n",
      "tensor([[ 74,  98],\n",
      "        [258, 346]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "src/column_parallelism.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "X = torch.tensor(\n",
    "    [\n",
    "        [0, 1, 2, 3],\n",
    "        [4, 5, 6, 7],\n",
    "    ]\n",
    ")\n",
    "\n",
    "A1 = torch.tensor(\n",
    "    [\n",
    "        [10],\n",
    "        [11],\n",
    "        [12],\n",
    "        [13],        \n",
    "    ]\n",
    ")\n",
    "\n",
    "A2 = torch.tensor(\n",
    "    [\n",
    "        [14],\n",
    "        [15],\n",
    "        [16],\n",
    "        [17],        \n",
    "    ]\n",
    ")\n",
    "\n",
    "Y1 = X @ A1\n",
    "Y2 = X @ A2\n",
    "\n",
    "print(Y1)\n",
    "print(Y2)\n",
    "\n",
    "Y = torch.cat([Y1, Y2], dim=1)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that the result **before and after parallelization is identical**.\n",
    "\n",
    "Now let’s move on to **Row Parallelism**.\n",
    "\n",
    "\n",
    "### Row Parallelism\n",
    "\n",
    "In Row Parallelism, the weight matrix `A` is split **horizontally**, and the partial results are **summed across devices**.\n",
    "\n",
    "As shown in the figure:\n",
    "- Both `X` and `A` are split:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 11,  15],\n",
      "        [ 95, 131]])\n",
      "tensor([[ 63,  83],\n",
      "        [163, 215]])\n",
      "tensor([[ 74,  98],\n",
      "        [258, 346]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "src/row_parallelism.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "X1 = torch.tensor(\n",
    "    [\n",
    "        [0, 1],\n",
    "        [4, 5],\n",
    "    ]\n",
    ")\n",
    "\n",
    "X2 = torch.tensor(\n",
    "    [\n",
    "        [2, 3],\n",
    "        [6, 7],\n",
    "    ]\n",
    ")\n",
    "\n",
    "A1 = torch.tensor(\n",
    "    [\n",
    "        [10, 14],\n",
    "        [11, 15],      \n",
    "    ]\n",
    ")\n",
    "\n",
    "A2 = torch.tensor(\n",
    "    [\n",
    "        [12, 16],\n",
    "        [13, 17],        \n",
    "    ]\n",
    ")\n",
    "\n",
    "Y1 = X1 @ A1\n",
    "Y2 = X2 @ A2\n",
    "\n",
    "print(Y1)\n",
    "print(Y2)\n",
    "\n",
    "Y = Y1 + Y2\n",
    "\n",
    "print(Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
